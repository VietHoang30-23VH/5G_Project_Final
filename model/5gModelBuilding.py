# -*- coding: utf-8 -*-
"""5G-Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SVokjrf9QidvTgiGApi_pFFgHVebn6_s

# **Khai b√°o c√°c th∆∞ vi·ªán s·ª≠ d·ª•ng**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelBinarizer
from sklearn.utils.class_weight import compute_class_weight
import seaborn as sns
import time
import datetime
import pytz
from joblib import dump
import warnings
import numpy as np
warnings.filterwarnings('ignore')

""":# **Xem n·ªôi dung c·ªßa dataset v√† ch·ªçn l·ªçc 25 tr∆∞·ªùng th√¥ng tin**

Xem c√°c c·ªôt c√≥ trong dataset
"""

file_path_full = '/content/drive/MyDrive/5G_model/Dataset_5G-NIDD_Encoded.csv'

df_full = pd.read_csv(file_path_full, low_memory=False)
df_full = df_full.drop(columns=['Unnamed: 0'], errors='ignore')
num_samples = df_full.shape[0]
num_columns = df_full.shape[1]

print(f"- S·ªë l∆∞·ª£ng m·∫´u: {num_samples}")
print(f"- S·ªë l∆∞·ª£ng c·ªôt: {num_columns}")
print("- Danh s√°ch c√°c c·ªôt:", df_full.columns.tolist())
cols = df_full.columns.tolist()
for i in range(0, len(cols), 15):
    print("  " + " | ".join(cols[i:i+15]))

"""Ch·ªçn c√°c tr∆∞·ªùng th√¥ng tin v√† t·∫°o 1 dataset m·ªõi t·ª´ c√°c tr∆∞·ªùng ƒë√≥"""

selected_columns = ['tcp', 'AckDat', 'sHops', 'Seq', 'RST', 'TcpRtt', 'REQ', 'dMeanPktSz','Offset', 'CON', 'FIN', 'sTtl', ' e        ', 'INT', 'Mean', 'Status', 'icmp',
 'SrcTCPBase', ' e d      ', 'sMeanPktSz', 'DstLoss', 'Loss', 'dTtl', 'SrcBytes', 'TotBytes', 'Label', 'Attack Type', 'Attack Tool']
existing_columns = [column for column in selected_columns if column in df_full.columns]
df_selected = df_full[existing_columns].copy()
output_path = '/content/drive/MyDrive/5G_model/Dataset_Using.csv'
df_selected.to_csv(output_path, index=False)

print(f"-- Result : Dataset m·ªõi ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i {output_path}")

"""# **Xem th√¥ng tin c·ªßa dataset m·ªõi v·ª´a t·∫°o**"""

file_path = '/content/drive/MyDrive/5G_model/Dataset_Using.csv'
df = pd.read_csv(file_path, low_memory=False)
df = df.drop(columns=['Unnamed: 0'], errors='ignore')
num_samples = df.shape[0]
num_columns = df.shape[1]
num_benign = df[df["Label"] == "Benign"].shape[0]
num_malicious = df[df["Label"] != "Benign"].shape[0]
num_dif_benign_malicious = num_benign/num_malicious

print(f"- S·ªë l∆∞·ª£ng m·∫´u: {num_samples}")
print(f"- S·ªë l∆∞·ª£ng c·ªôt: {num_columns}")
print("- Danh s√°ch c√°c c·ªôt:", df.columns.tolist())
# In m·ªói h√†ng 10 c·ªôt
cols = df.columns.tolist()
for i in range(0, len(cols), 15):
    print("  " + " | ".join(cols[i:i+15]))
print(f"- S·ªë l∆∞·ª£ng benign traffic: {num_benign}")
print(f"- S·ªë l∆∞·ª£ng malicious traffic: {num_malicious}")
print(f"- ƒê·ªô ch√™nh l·ªánh gi·ªØa benign v√† malicious: {num_dif_benign_malicious}")

print(df.info())

df["Attack Type"].value_counts()

"""So s√°nh"""

percent_benign = (num_benign/num_samples) * 100
percent_malicious = (num_malicious/num_samples) * 100
labels = ['Benign', 'Malicious']
sizes = [percent_benign, percent_malicious]
colors = ['#90EE90','#20B2AA']
explode = (0,0)

plt.figure(figsize=(4,3))
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.2f%%',startangle=90)
plt.axis('equal')
plt.title('Comparison of Benign and Malicious')
plt.show()

"""# **Split data v√† x·ª≠ l√≠ d·ªØ li·ªáu b·ªã thi·∫øu**
- X√≥a d·ªØ li·ªáu b·ªã thi·∫øu
- X√≥a c√°c c·ªôt IP

=> Gi·ªØ l·∫°i khi·∫øn m√¥ h√¨nh d·ªÖ ph·ª• thu·ªôc thay v√¨ ph√¢n t√≠ch l∆∞u l∆∞·ª£ng.
- Th·ª±c hi·ªán Pearson Correlation: lo·∫°i b·ªè c√°c ƒë·∫∑c tr∆∞ng d∆∞ th·ª´a, c√≥ nghƒ©a l√† c√°c ƒë·∫∑c tr∆∞ng t∆∞∆°ng quan cao, mang th√¥ng tin gi·ªëng nhau. Ch·ªâ gi·ªØ l·∫°i m·ªôt ƒë·∫∑c tr∆∞ng duy nh·∫•t.
- Th·ª±c hi·ªán ph√¢n t√≠ch ANOVA F-scores: ƒë∆∞a ra 25 tr∆∞·ªùng c√≥ ·∫£nh h∆∞·ªüng cao nh·∫•t ƒë·∫øn k·∫øt qu·∫£.

**Chia d·ªØ li·ªáu train/set**
"""

# Handle missing values
print("\nüßπ Handling missing values...")
# Replace infinities with NaN so they can be caught by the imputer
df.replace([np.inf, -np.inf], np.nan, inplace=True)

# Fill NaN values for numeric columns with median
numerical_columns = df.select_dtypes(include=np.number).columns
df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].median())

# Fill NaN values for categorical columns with mode
categorical_columns = df.select_dtypes(exclude=np.number).columns
for col in categorical_columns:
    if df[col].isna().sum() > 0:
        df[col] = df[col].fillna(df[col].mode()[0])

# Verify NaN values are handled
print("\nüìä Missing values after preprocessing:")
print(df.isna().sum().sum(), "total NaN values remaining")

df['Multiclass_Label'] = df['Label'] + "_" + df['Attack Type'] + "_" + df['Attack Tool']
X = df.drop(columns=["Label", "Attack Type", "Attack Tool", "Multiclass_Label"])
y = df["Multiclass_Label"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)
print("Train set size:",len(X_train))
print("Ph√¢n ph·ªëi Label trong Train set:")
print(y_train.value_counts(normalize=True))
print("-"*50)
print("Test set size:",len(X_test))
print("Ph√¢n ph·ªëi Label trong Test set:")
print(y_test.value_counts(normalize=True))

"""**Ki·ªÉm tra d·ªØ li·ªáu b·ªã thi·∫øu**"""

def check_missing_values(df):
    missing_values = df.isnull().sum()
    missing_percentage = (missing_values / len(df)) * 100

    missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})
    missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values(by='Missing Values', ascending=False)

    if missing_df.empty:
        print("Kh√¥ng c√≥ gi√° tr·ªã b·ªã thi·∫øu trong dataset.")
    else:
        print(missing_df)
        # V·∫Ω bi·ªÉu ƒë·ªì tr·ª±c quan h√≥a d·ªØ li·ªáu b·ªã thi·∫øu
        plt.figure(figsize=(10, 5))
        missing_df['Missing Values'].plot(kind='bar', color='red')
        plt.title('S·ªë l∆∞·ª£ng gi√° tr·ªã b·ªã thi·∫øu trong m·ªói c·ªôt')
        plt.xlabel('C·ªôt')
        plt.ylabel('S·ªë l∆∞·ª£ng')
        plt.xticks(rotation=45)
        plt.show()

print("--- Ki·ªÉm tra ·ªü trainning set ---")
check_missing_values(X_train)

for col in ['dTtl', 'SrcTCPBase', 'sTtl', 'sHops']:
    X_train[col].fillna(-1, inplace=True)
    X_test[col].fillna(-1, inplace=True)
check_missing_values(X_train)

"""# **X√¢y d·ª±ng m√¥ h√¨nh random forest**

T·∫°o model
"""

from google.colab import files
# Kh·ªüi t·∫°o m√¥ h√¨nh
print("\nüîß Training Random Forest model...")
# Ghi nh·∫≠n th·ªùi gian b·∫Øt ƒë·∫ßu
start_time = time.time()
print("\n‚öñÔ∏è Computing class weights...")
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(zip(np.unique(y_train), class_weights))
# Train the model with class weights
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=30,
    min_samples_split=2,
    min_samples_leaf=1,
    class_weight=class_weight_dict,
    random_state=42
)
# Hu·∫•n luy·ªán m√¥ h√¨nh
rf_model.fit(X_train, y_train)
# Th√™m t·∫≠p validation
# Ghi nh·∫≠n th·ªùi gian k·∫øt th√∫c
end_time = time.time()
execution_time = end_time - start_time

print(f"‚è±Ô∏è Th·ªùi gian hu·∫•n luy·ªán: {execution_time:.2f} gi√¢y")
dump(rf_model, 'my_rf_model.joblib')

"""Ti·∫øn h√†nh ƒë√°nh gi√° model v√† t·∫£i xu·ªëng model"""

#files.download('my_rf_model.joblib')
print("Download model successful")
y_pred = rf_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

lb = LabelBinarizer()
y_test_binarized = lb.fit_transform(y_test)
y_pred_proba = rf_model.predict_proba(X_test)

if isinstance(y_pred_proba, list):
    y_pred_proba = y_pred_proba[0]

try:
    roc_auc = roc_auc_score(y_test_binarized, y_pred_proba, average="weighted", multi_class="ovr")
except ValueError:
    roc_auc = None

print("\nüéØ ƒê√°nh gi√° m√¥ h√¨nh Random Forest:")
print(f"- Accuracy     : {accuracy:.4f}")
print(f"- Precision    : {precision:.4f}")
print(f"- Recall       : {recall:.4f}")
print(f"- F1-score     : {f1:.4f}")
print(f"- ROC AUC      : {roc_auc:.4f}" if roc_auc else "- ROC AUC      : Kh√¥ng t√≠nh ƒë∆∞·ª£c v·ªõi d·ªØ li·ªáu n√†y")
print(f"- Execution Time: {execution_time:.2f} gi√¢y")

# Generate detailed classification report
print("\nüìù Detailed Classification Report:")
print(classification_report(y_test, y_pred))

"""ƒê√°nh gi√° b·∫±ng Confusion Matrix"""

# L·∫•y danh s√°ch c√°c nh√£n duy nh·∫•t
labels = np.unique(np.concatenate((y_test, y_pred)))

# T·∫°o confusion matrix
cm = confusion_matrix(y_test, y_pred, labels=labels)

# T·∫°o DataFrame cho confusion matrix ƒë·ªÉ d·ªÖ hi·ªÉn th·ªã
cm_df = pd.DataFrame(cm, index=labels, columns=labels)

# V·∫Ω heatmap c·ªßa confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix', fontsize=14)
plt.ylabel('Actual Labels', fontsize=12)
plt.xlabel('Predicted Labels', fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(rotation=0, fontsize=10)
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()
print('='*200)
# T·∫°o normalized confusion matrix (t·ª∑ l·ªá ph·∫ßn trƒÉm)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
cm_normalized_df = pd.DataFrame(cm_normalized, index=labels, columns=labels)

# V·∫Ω heatmap c·ªßa normalized confusion matrix
plt.figure(figsize=(12, 10))
sns.heatmap(cm_normalized_df, annot=True, fmt='.1%', cmap='Blues', cbar=False)
plt.title('Percentage Confusion Matrix', fontsize=14)
plt.ylabel('Actual Labels', fontsize=12)
plt.xlabel('Predicted Labels', fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(rotation=0, fontsize=10)
plt.tight_layout()
plt.savefig('normalized_confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

# Data t·ª± input
# sample_dict = [
#     {
#         'tcp': 0, 'AckDat': 0, 'sHops': 1, 'Seq': 47138, 'RST': 0, 'TcpRtt': 0, 'REQ': 1,
#         'dMeanPktSz': 0, 'Offset': 19272380, 'CON': 0, 'FIN': 0, 'sTtl': 63, ' e        ': 1,
#         'INT': 0, 'Mean': 2.582307, 'Status': 1, 'icmp': 0, 'SrcTCPBase': -1, ' e d      ': 0,
#         'sMeanPktSz': 42, 'DstLoss': 0, 'Loss': 0, 'dTtl': -1, 'SrcBytes': 84, 'TotBytes': 84
#     },
#     {
#         'tcp': 1, 'AckDat': 0, 'sHops': 10, 'Seq': 20000, 'RST': 0, 'TcpRtt': 12, 'REQ': 0,
#         'dMeanPktSz': 500, 'Offset': 10000, 'CON': 0, 'FIN': 0, 'sTtl': 64, ' e        ': 0,
#         'INT': 20, 'Mean': 1.2, 'Status': 0, 'icmp': 0, 'SrcTCPBase': 0, ' e d      ': 0,
#         'sMeanPktSz': 512, 'DstLoss': 0, 'Loss': 0, 'dTtl': 64, 'SrcBytes': 1024, 'TotBytes': 2048
#     },
#         {
#         'tcp': 1, 'AckDat': 0.024088, 'sHops': 1, 'Seq': 17276, 'RST': 0, 'TcpRtt': 0.026046, 'REQ': 0,
#         'dMeanPktSz': 66, 'Offset': 10098024, 'CON': 0, 'FIN': 1, 'sTtl': 63, ' e        ': 1,
#         'INT': 0, 'Mean': 0, 'Status': 1, 'icmp': 0, 'SrcTCPBase': 4238886840, ' e d      ': 0,
#         'sMeanPktSz': 66, 'DstLoss': 0, 'Loss': 0, 'dTtl': 59, 'SrcBytes': 66, 'TotBytes': 132
#     }
# ]
# sample_df = pd.DataFrame(sample_dict)
# sample_df = sample_df[X_train.columns]
# sample_df = sample_df.fillna(0)

# Data l·∫•y csv ch·ªâ ƒë·ªß 28
csv_data = pd.read_csv('/content/drive/MyDrive/5G_model/4_Records_Test.csv')

# Lo·∫°i b·ªè 3 c·ªôt kh√¥ng c·∫ßn thi·∫øt
columns_to_drop = ['Label', 'Attack Tool', 'Attack Type']
csv_data = csv_data.drop(columns=columns_to_drop, errors='ignore')

# L·∫•y c√°c c·ªôt ph√π h·ª£p v·ªõi t·∫≠p hu·∫•n luy·ªán
csv_data = csv_data[X_train.columns]

# X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu
csv_data = csv_data.fillna(0)

# Ti·∫øp t·ª•c v·ªõi d·ª± ƒëo√°n
y_preds = rf_model.predict(csv_data)
# Hi·ªÉn th·ªã k·∫øt qu·∫£
print("\nK·∫øt qu·∫£ d·ª± ƒëo√°n c√°c m·∫´u:")
for idx, y_pred in enumerate(y_preds):
    try:
        label, attack_type, tool = y_pred.split('_')
    except:
        label, attack_type, tool = y_pred, 'Unknown', 'Unknown'
    current_time = datetime.datetime.now(pytz.timezone('Etc/GMT-7')).strftime("%H:%M:%S-%d/%m/%Y")
    print(f"M·∫´u {idx+1} with [{current_time}]:")
    print(f"  - Label        : {label}")
    print(f"  - Attack Type  : {attack_type}")
    print(f"  - Attack Tool  : {tool}\n")

# ƒê·ªçc file CSV
csv_data = pd.read_csv('/content/drive/MyDrive/5G_model/Dataset_5G-NIDD_Encoded.csv')

# Lo·∫°i b·ªè 3 c·ªôt kh√¥ng c·∫ßn thi·∫øt
columns_to_drop = ['Label', 'Attack Tool', 'Attack Type']
csv_data = csv_data.drop(columns=columns_to_drop, errors='ignore')

# Ch·ªâ ch·ªçn c√°c c·ªôt c·∫ßn thi·∫øt cho m√¥ h√¨nh
csv_data = csv_data[X_train.columns]

# X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu
csv_data = csv_data.fillna(0)

# Ti·∫øp t·ª•c v·ªõi d·ª± ƒëo√°n
y_preds = rf_model.predict(csv_data)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
print("\nK·∫øt qu·∫£ d·ª± ƒëo√°n c√°c m·∫´u:")
for idx, y_pred in enumerate(y_preds):
    try:
        label, attack_type, tool = y_pred.split('_')
    except:
        label, attack_type, tool = y_pred, 'Unknown', 'Unknown'
    current_time = datetime.datetime.now(pytz.timezone('Etc/GMT-7')).strftime("%H:%M:%S-%d/%m/%Y")
    print(f"M·∫´u {idx+1} with [{current_time}]:")
    print(f"  - Label        : {label}")
    print(f"  - Attack Type  : {attack_type}")
    print(f"  - Attack Tool  : {tool}\n")